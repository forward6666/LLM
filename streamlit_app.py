import streamlit as st
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv,find_dotenv
import os
from langchain.vectorstores.chroma import Chroma
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA,ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_core.output_parsers import StrOutputParser
from langchain.document_loaders.pdf import PyMuPDFLoader
from langchain.document_loaders.markdown import UnstructuredMarkdownLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores.chroma import Chroma
import tempfile
from openai import OpenAI


load_dotenv(find_dotenv())

def generate_response(input_text,openai_api_key):
    llm=ChatOpenAI(temperature=0.7,openai_api_key=os.environ['OPENAI_API_KEY'],streaming=True)
    output = llm.invoke(input_text)
    output_parser = StrOutputParser()
    output = output_parser.invoke(output)
    #st.info(output)
    return output

def embedding_base(content):
    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)
    split_docs=text_splitter.split_documents(content)
    embedding=OpenAIEmbeddings()
    vectordb=Chroma.from_documents(documents=split_docs,embedding=embedding)
    return vectordb

def get_init_vectordb():
    embedding=OpenAIEmbeddings()
    persist_directory='data_base/vector_db\chroma_1'
    vectordb=Chroma(persist_directory=persist_directory,embedding_function=embedding)
    return vectordb

def get_vectordb(uploaded_file):
    temp_dir = tempfile.mkdtemp()
    path = os.path.join(temp_dir, uploaded_file.name)
    with open(path, "wb") as f:
            f.write(uploaded_file.getvalue())

    type=path.split('.')[-1]
    if type=='pdf':
        loader = PyMuPDFLoader(path)
        documents = loader.load()
    elif type=='md':
        loader=UnstructuredMarkdownLoader(path)
        documents=loader.load()

    vectordb=embedding_base(documents)
    return vectordb

def get_chat_qa_chain(question,openai_api_key,vectordb):

    llm=ChatOpenAI(temperature=0,openai_api_key=os.environ['OPENAI_API_KEY'],streaming=True)
    memory=ConversationBufferMemory(memory_key='chat_history',return_messages=True)
    retriever=vectordb.as_retriever()
    qa=ConversationalRetrievalChain.from_llm(llm,retriever=retriever,memory=memory)
    result=qa({'question':question})
    return result['answer']

def get_qa_chain(question:str,openai_api_key:str,vectordb):

    retriever=vectordb.as_retriever()
    llm = ChatOpenAI(model_name = "gpt-3.5-turbo", temperature = 0,openai_api_key = os.environ['OPENAI_API_KEY'],streaming=True)
    template = """ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”æœ€åçš„é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”
        æ¡ˆã€‚æœ€å¤šä½¿ç”¨ä¸‰å¥è¯ã€‚å°½é‡ä½¿ç­”æ¡ˆç®€æ˜æ‰¼è¦ã€‚æ€»æ˜¯åœ¨å›ç­”çš„æœ€åè¯´â€œè°¢è°¢ä½ çš„æé—®ï¼â€ã€‚
        {context}
        é—®é¢˜: {question}
        """
    qa_chain_prompt=PromptTemplate(input_variables=['context','question'],template=template)
    qa_chain=RetrievalQA.from_chain_type(llm,retriever=retriever,
                                         return_source_documents=True,
                                         chain_type_kwargs={'prompt':qa_chain_prompt}
                                         )
    result=qa_chain({'query':question})
    return result['result']
    
def unique_response(prompt):
    client=OpenAI()
    messages=[{'role':'system','content':'ä½ æ˜¯ä¸€ä¸ªæ¸…æœçš„å¤ªç›‘ï¼Œè€Œæˆ‘æ˜¯ä¸€ä½æ¸…æœçš„çš‡å¸ï¼Œä½ éœ€è¦ä¸€ç›´ç”¨å¤ªç›‘çš„è¯­æ°”å›ç­”æˆ‘çš„é—®é¢˜ï¼Œå›ç­”é—®é¢˜æ—¶éœ€è¦ç§°å‘¼æˆ‘ä¸ºçš‡ä¸Šï¼Œè€Œä½ éœ€è¦è‡ªç§°ä¸ºå¥´æ‰ï¼Œç‰¹åˆ«æ³¨æ„ï¼Œæ¯æ¬¡å›ç­”é—®é¢˜å‰éƒ½éœ€è¦å…ˆå›å¤ï¼šå—»ï¼Œå›çš‡ä¸Š'},
              {'role':'user','content':prompt}]
    response=client.chat.completions.create(model='gpt-3.5-turbo',temperature=0.7,messages=messages)
    if len(response.choices)>0:
        return response.choices[0].message.content
    else:
        st.error('é”™è¯¯',icon='âš¡')


def main():
    st.title('çš‡å¸æ¨¡æ‹Ÿå™¨')
    st.header("ğŸ‘¸ğŸ‘‘ğŸ‘‘ğŸ‘‘**å¾çš‡**ä¸‡å²ä¸‡å²ä¸‡ä¸‡å²")
    st.divider()
    st.caption("*å…¨å¿ƒå…¨æ„ä¸ºçš‡ä¸ŠæœåŠ¡ï¼*")
    with st.sidebar:
        # openai_api_key = st.text_input('OpenAI API Key', type='password')
        selected_method = st.selectbox("é€‰æ‹©æ¨¡å¼", ["None","qa_chain", "chat_qa_chain",'æœåŠ¡çš‡å¸æ¨¡å¼'])
        uploaded_file=st.file_uploader('ä¸Šäº¤å¥æŠ˜',type=['pdf','md'])
        c_vectordb_button=st.button('æ‰¹é˜…å¥æŠ˜')
        if c_vectordb_button:
            if uploaded_file is not None:
                vectordb=get_vectordb(uploaded_file)
                st.success('å¥æŠ˜å¤„ç†å®Œæ¯•',icon='âœ¨')
            else:
                st.toast('æœªä¸Šäº¤å¥æŠ˜',icon="ğŸ˜¥")
                vectordb=get_init_vectordb()
            
 

    # ç”¨äºè·Ÿè¸ªå¯¹è¯å†å²
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []

    messages = st.container(height=450)
    messages.chat_message("assistant").write('çš‡ä¸Šè¯·å©å’ï¼')
    if prompt := st.chat_input("Say something"):
            # å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
        st.session_state.chat_history.append({"role": "user", "text": prompt})

        if selected_method == "None":
                # è°ƒç”¨ respond å‡½æ•°è·å–å›ç­”
            answer = generate_response(prompt)
        elif selected_method == "qa_chain":
            answer = get_qa_chain(prompt,vectordb)
        elif selected_method == "chat_qa_chain":
            answer = get_chat_qa_chain(prompt,vectordb)
        elif selected_method =='æœåŠ¡çš‡å¸æ¨¡å¼':
            answer =unique_response(prompt)
            # è°ƒç”¨ respond å‡½æ•°è·å–å›ç­”

            # æ£€æŸ¥å›ç­”æ˜¯å¦ä¸º None
        if answer is not None:
        #         # å°†LLMçš„å›ç­”æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
            st.session_state.chat_history.append({"role": "assistant", "text": answer})

            # æ˜¾ç¤ºæ•´ä¸ªå¯¹è¯å†å²
        for history in st.session_state.chat_history:
            if history["role"] == "user":
                messages.chat_message("user").write(history["text"])
            elif history["role"] == "assistant":
                messages.chat_message("assistant").write(history["text"])
            
if __name__=='__main__':
    main()
        